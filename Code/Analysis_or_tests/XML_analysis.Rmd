---
title: "XML data analysis"
output: 
  html_notebook: 
    toc: yes
---

```{r setup}
knitr::opts_chunk$set(include = FALSE)
library(XML)
library(dplyr)
library(ggplot2)
```

# Main issues
1. Data is stored in a series of xml files
2. Each xml file is many xml documents stitched together
3. Extracting relatvent data from each xml document. 
4. These problems are associated with the XML data, before Jan2002 a different data format is used. (SGML & APS?) <https://bulkdata.uspto.gov/> (scroll down to bibliographic data)

# Introduction: The data source

There is no known clean / table version of this dataset that I could find. The data is described as the [USPTO Grant Bibliographic](http://www.uspto.gov/learning-and-resources/electronic-data-products/issued-patents-patent-grants-data-products):

* Files are hosted in two places [here](https://bulkdata.uspto.gov/) and [here](http://patents.reedtech.com/pgrbbib.php)
* Each file is approximately 5MB compressed, representing 1 week of patent issues
* The standards for encoding this data change fairly frequently, since 2002 has been in somme kind of XML standard. 
    * Documentation of standards found [here](http://www.uspto.gov/learning-and-resources/xml-resources/xml-resources-retrospective)
* Each XML file is many (4000+) XML documents stitched together


The data is stored in a series of xml documents (representing patents issued that week). We are using the latest release "ipg150428" released 2015-04-28 as a test document to see how to read and store relevant data. The files are large, approxmately the same size, the test file is 168.4MB.

When attempting to use the xml2 package to read the file into xml we get the following error:
```{r xml error, eval=F}
xml <- xml2::read_xml("../DataFiles/testdata.xml")
```
By reading the document into R and parsing for the delarations using grep we can see that the problem is **multiple xml declarations** in the document. This implies the file is several thousand xml documents stitched together. . 
```{r multiple docs}
lines <- readLines("../DataFiles/testdata.xml")
xmltags <- grep("<?xml", lines)
num_docs <- length(xmltags)
```
Above we cal see there are 7052 xml documents within our xml file, however storing all this as a large character vector creates a large object `r pryr::object_size(lines)/(8*10^6)` MB big. Considering the amount of files needing to be processed there is likely better ways of dealing with this problem. 


# XML structure
We can look at only the first document in the file to get a sense for the structure of the data. 

```{r xml structure}
xml1 <- paste(lines[xmltags[1]:(xmltags[2]-1)], collapse = "")
xml2::xml_structure(xml2::read_xml(xml1), indent = 4)
```
The structure above is not flat so it is not easy to parse. 

The most important piece of the structure here is that all the citations are under the node <us-references-cited> with the tag <us-citation>. But the information stored in these citations are in a tree structure with <patcit>, <category>, and <classification-national> as three branches before the leaf nodes. Finally the final citation is a different structure than the others with only 2 fields <othercit>/<category> and its index in the list. 

# Data frame of one document's citations

The below creates a data frame from the citation documentation of 1 document. 
```{r, message=FALSE, warning=FALSE}
parse_xml <- function(text.xml.doc) {
  doc1_xml <- xmlTreeParse(text.xml.doc, asText = T, useInternalNodes = T)
  
  # Creates a nested list of xml structure up to 3 layers deep
  f <- function(L1) {
    # Returns the name-value pair of first attribute
    a <- xmlAttrs(L1)
    b <- xmlSApply(L1, function(L2) {
      d <- xmlValue(L2)
      e <- xmlSApply(L2, xmlValue)
      ifelse(length(e) == 0, return(d), return(e))
    })
    
    # Convert matrix to named list of characters so unlist retains name information later
    if (class(b) == "matrix") {
      nam <- rownames(b)
      b <- as.character(b)
      names(b) <- nam
    }
    if (!is.null(a)) b <- c(id = a,b)
    return(b)
  }
  
  # Evaluates to functions above to create nested xml structure of citations for each patent
  citations_listed <- xpathSApply(doc1_xml, 
                                  "//us-references-cited/us-citation", 
                                  fun = function(L0) xmlSApply(L0, f))
  # Flattens the above structure
  citations.flat <- lapply(citations_listed, unlist)
  
  # Exception handling in case no citations for a document
  if (length(citations.flat) == 0) return(NULL)
  
  # Combines all citations into one data frame
  df <- data.frame(t(citations.flat[[1]]), stringsAsFactors = F)
  for (i in 2:(length(citations.flat))) {
    df <- bind_rows(df, as.data.frame(t(citations.flat[[i]]), stringsAsFactors = F))
  }
  
  doc.id <- xpathSApply(doc1_xml,
                        "//us-bibliographic-data-grant/publication-reference/document-id/doc-number",
                        xmlValue)

  # Return the df as a named element of a list
  return.object <- list()
  return.object[[get("doc.id")]] <- df
  return(return.object)
}

df <- parse_xml(xml1)
```
Above we can see the patent citation information for the first document in one data frame. There are some anomalies in the naming convention for the first document where 3 of the variables were named with '-' instead of '.' for the others. And the final citation is quite different with only 3 variables category, id, and lengthy text. We will have to see how this compares to other documents. 

Below we run the "parse_xml" function defined above over one whole document. It takes 9 minutes to evaluate this file and experiences approximately 16 errors due to premature end of data in tags and extra content at the end of the document
```{r loop through each document}
t <- Sys.time()
citation.list <- NULL
for (i in 1:(length(xmltags)-1)) {
  text.xml.doc <- paste(lines[xmltags[i]:(xmltags[i + 1]-1)], collapse = "")
  citation.doc <- try(parse_xml(text.xml.doc))
  citation.list <- c(citation.list, citation.doc)
}
save(citation.list, file = "../DataFiles/citations.file1.dat")
Sys.time() - t
```
## Analysing 

Looking only at the number of citations each patent recieves we can see a large range which loosely follows a normal distribution if scaled logarithmically. The majority of patents have between around 5-80 citations. 
```{r number of citations}
#load("../DataFiles/citations.file1")
num.citations <- sapply(citation.list, nrow) %>% unlist
ggplot(data = as.data.frame(num.citations), aes(x = num.citations)) + 
    geom_density() +
    scale_x_log10()
summary(num.citations)
```

Looking at the different variables extracted we can see some cleaning will be necessary. The number of patents in this file is `r length(citation.list)` but the most frequent occurance of a variable is less than 6000. There is also variation in naming convention e.g. "doc-number" and "doc.number". There are a few variables with very low numbers of occurances such as "o" and "img"
```{r number of variables}
sapply(citation.list, names) %>% unlist() %>% table()
```

```{r}
t <- Sys.time()
citation.df <- data.frame()
for (i in 1:length(citation.list)) {
    citation.list[[i]]$patent.id <- names(citation.list[i])
    citation.df <<- bind_rows(citation.df, citation.list[[i]])
}
names(citation.df) <- make.names(names(citation.df))
save(citation.df, file = "../DataFiles/citation.df1.dat")
Sys.time() - t
```

