---
title: "Progress so far"
author: "Alun"
date: "25 July 2016"
output: html_document
---

Sections:

 * Processing Data
 * Cleaning Data
 * Reproducing Valverde
 
## Processing Data

### Scripts involved
 
 * Parse_File_statebased.R
 * Parse_Directory3.R
 * OldCode/extract_basic.R, OldCode/Extract_citation_ids.R, OldCode/Extract_citation_ids2.R, OldCode/extract_ids_script.R, OldCode/Parse_Directory.R, OlCode/Parse_Directory2.R, OldCode/parse_file_text.R, OldCode/parse_xgml.R
 
### Problems found:

 * Tried to use the "XML" package based on "XML" C library but when scaling it discovered a memory leak, rewrote in "xml2"
 * Wrote a second script to parse the text format but when analysing there were systematic differences between this and the xml parsing. So wrote a script which can parse all 3 formats with the same method. 
 * Parsing all of the data took about 5 days. Only patent numbers, issue dates (and in-degree order) were parsed to keep the script simple and parsing times down. 
 * Format of the parsed data: files split into years, two files per year patents and citations. 
     * Patent file contains one row per patent, the order of that patent and the date in which it was granted. 
     * Citation file contains one row per citation (many rows per patent), the patent number of the parent patent, the patent number of the citation and the date in which the citation was issued. 
 
### Questions / future work:
 
 * Can re-parse with different information extracted, what variables would be useful to extract here. 
 * I have included foreign patent citations in my parsing, I believe valverde did not (from the analysis), so when re-parsing should I include foreign patents or not?
 
## Cleaning Data 

### Scripts involved

  * Cleaning.R
  * Analysis_Testing_Extraction_and_Cleaning.Rmd
  * Analysis_Testing_Extraction_and_Cleaning.nb.html
 
### Cleaning Steps taken: 
 
  * Removed duplicate entries
  * Parse Date into a date format
      * For Citation data only the month and year is recorded so artificially set the day to be "01". 
      * When parsed into date format some NA values introduced through parsing errors, these are low in number ~ 0-20 per year. 
  * Made patent ids consistent
      * For Patent ids there was extra '0's for padding and additional digit for text parsed ids. 
      * For citation ids the class code at the start was "D. 090" style rather than "D090" style. 
      
### Compare results to other sources:

  * In 2001 both sgml and text are present, comparing both parsing we see text yielding 94 more citations with no sgml citations missing from the text parse.
  * Comparing lst files (which document the patents expected to be present in xml files), we see differences appearing only after ~ 2008 but not too significantly. 
  * 1976 and 2015 are anomylous, 2015 because 2 months were unable to be downloaded, 1976 for unknown reasons. 
  
### Future Work / Questions

  * Still haven't fully cleaned the citation data, with preparation to build interaction matrix. 
  * Put the data in a database? (currently stored in csvs)
  
## Reproducing Valverde 

  * I have reproduced the two figures from valverde. 
  
### Figure 1
  * Figure 1 shows a similar pattern, there is an error in the scale labels but it seems that around year 2000, there are some obvious differences, valverde never gets above 1.75e5 whereas ours reaches that point at the year 2000. 
    * Based on the analysis in the previous section we can be fairly confident that our analysis is more accurate than valverde's and that the difference is likely due to parsing errors that we encountered as a difference between using the xml package and custom function. 
  * The inset fit a linear plot on the CDF of patent counts per year. We reproduced that plot and found a power law gradient of 1.23, rather than the 1.45 valverde produced. 
    * The plot looks like it isn't a linear fit but still has a .98 rsquared. 
    * We can approximate the results of valverde by taking a subset of the data, removing the new data that valverde didn't have and the early part of the data, which diverges from the linear fit substantially in their graph. (1985:2004)

### Figure 2
  
  * We reproduced figrue 2 and created approximate linear fit over the linear sections and included 2012 as an additional year. 
  * The gradients are higher than those in valverde fig2. which is consistent with the idea valverde didn't include foreign patents. 
  * We also reproduced that plot with normalised frequencies instead of counts. 
  * We also reproduced this plot using binned histograms to reduce the increased randomness in the tail however this introduced problems due to the discrete nature of the distribution. 
  * Finally we used the "poweRlaw" package to fit a variety of distributions to the data and found lognormal and powerlaw to have equivalent "goodness of fit" characteristed by kolmogorov-smirnov distance. 
    * However using a bootstrapping method yielded 0 p values for both of these distributions.
    
### Furether work / questions
 
  * How much more rigorous do these reproductions need to be?
  * Especially the fitting of distributions, should I spend the effort trying to fit these better?
    * I'm not 100% sure how this is working, it might be testing the whole distribution as a fit, so cutting off the extremes could yield non-0 p values, is this even a reasonable thing to do?
    * I am not sure how to fit an extended-power-law distribution, the paper doesn't talk about methods and a cursory scan of the literature didn't find much of significance.
    
    
## State of Project

### Current work

  * Finishing cleaning the citation patent numbers so an adjacency matrix can be properly constructed. 
  * Reading more thoroughly the paper on identifying the distribution, to do a better job quantifying the underlying distribtion. 
  * Learning how to put the data in a database to make the other steps easier. 

### Future work

  * Timescale, considering how far along we are at this point how should we progress? 
  * It may be an idea to abandon the more rigerous reproduction of valverde because we need to get our novel work done. 
    * How much to focus on making current analysis water-tight
    * Reparsing the data? Foreign patents / other variables, if we want to include additional factors we need to identify which ones to parse, important factors could include foreign patents, classification codes, wheter citations are "cited by examiner" or not, time period between patent application and issue etc. 
      * We will only have time to do this once more so this should be thought out and understand exactly what we are tryint to get from this effort. 
  
### Ideas for evolution analysis
  
  * With the data of just patents, citations and citation dates we can characterise each patent as a naive time series. I.e. characterise each citaiton as a time from patent issue date and analyse these distributions. 
      * How would you go about analysing these distributions? Could you apply some machine learning to predict the overall number of citations given the start of the time series?
      * Cleaning this data:
          * data would have to be normalised for by year, because of the increasing number of patents and even more increasing number of citations.
          * We may also want to apply say a 20 year window over the data (only look at the first 20 years of each patent and ignore patents which are not 20 years old?), this would get rid of the effect that some of the patents haven't been around long enough to become successful yet. 
  * We could look to improving on this naive approach with extra variables such as the degree of the node being cited at that time?

